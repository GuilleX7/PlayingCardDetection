{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "source": [
    "# Setting up\n",
    "\n",
    "First, we will define some global variables and utilities that will be helpful for us. We will consider two types of images:\n",
    "\n",
    "### Template images\n",
    "Those images are meant to fulfill some special requirements. By default, they are contained in the `template` folder. They contain the sample cards that will be used to perform matching in query images. Please note that you can replace the template images and change the value of all the variables declared in the following block to make this notebook work with other card decks, as long as these conditions are met:\n",
    "\n",
    "- Every template image must contain only one card\n",
    "- Card must be a rectangle\n",
    "- Card must be in foreground and can be easily distinguished from the background\n",
    "- At least one corner must have some special symbol on it. It doesn't matter whether it contains the card's suit or rank, as it is only used to calculate correct orientation. Note that this rule is met in a standard french 52-card deck, though we have two corners that match this condition. This is not a problem, as both corners can be used as the top-left corner (cards are moreless symmetric, and we've got some tricks to handle this that we'll see later).\n",
    "\n",
    "Notice that `card_suits` and `card_ranks` represent the possible suits and ranks that can exist in your deck, and rank don't even have to be numeric. All your images in the `template` folder must follow the naming pattern `{card_suit}_{card_rank}`, unless you modify the `get_card_name()` method to a more convenient one for you.\n",
    "\n",
    "### Query images\n",
    "They can be -literally- any image. They are contained in the `query` folder by default. Let me insist on this: you can put any image here, but you will get better results if they follow this condition:\n",
    "\n",
    "- Do not overlap cards. Overlapping cards might cause that the card below is not isolated as a full rectangle, and therefore not considered a card.\n",
    "\n",
    "Please notice that both types of images can be any size and colour, as they are gray scaled and standardized before we use them. This also applies to query images, so there's no problem with arbitrary images :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Globals\n",
    "###############################################################################\n",
    "\n",
    "\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Variables you would like to change :)\n",
    "card_suits = [\"clubs\", \"diamonds\", \"hearts\", \"spades\"]\n",
    "card_max_rank = 13\n",
    "template_card_image_extension = \"jpg\"\n",
    "template_path = \"../template/\"\n",
    "training_card_image_extension = \"png\"\n",
    "training_path = \"../training/\"\n",
    "query_path = \"../query/\"\n",
    "\n",
    "# Get the standard name for a card given its suit and rank\n",
    "def get_card_name(card_suit, card_rank):\n",
    "    return f\"{card_suit}_{card_rank}\"\n",
    "\n",
    "# Get the path to the template card given its name\n",
    "def get_template_card_path(card_name):\n",
    "    return f\"{template_path}{card_name}.{template_card_image_extension}\"\n",
    "\n",
    "# Get the path to the trained card given its name\n",
    "def get_trained_card_path(card_name):\n",
    "    return f\"{training_path}{card_name}_trained.{training_card_image_extension}\"\n",
    "\n",
    "# Get the path to the card's features file given its name\n",
    "def get_features_file_path(card_name):\n",
    "    return f\"{training_path}{card_name}.npz\"\n",
    "\n",
    "# Get the path to a query image given its name\n",
    "def get_query_image_path(file_name):\n",
    "    return f\"{query_path}{file_name}\"\n",
    "\n",
    "# Get a list of card names\n",
    "cards = [get_card_name(card_suit, card_rank)\n",
    "         for card_suit in card_suits for card_rank in range(1, card_max_rank + 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Card detection & feature extraction\n",
    "\n",
    "In the following block, we will iterate over all template images and we'll try to isolate the corresponding card from the background (depending on the template name). Also, we'll compute and extract features from them to perform feature matching later.\n",
    "\n",
    "The output will help you have a brief idea of what's going on: each card is processed in two stages, the first one crops and standardizes the card, and the second one computes and extracts features. Any stage can be skipped depending on whether the card is already standardized or whether the features have been already extracted and saved in a file. This way we save our precious time :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Card & feature extraction\n",
    "###############################################################################\n",
    "\n",
    "\n",
    "# So you got here. This is where all magic happens. Don't worry, I'll explain\n",
    "# everything for you to understand what's happening behind the scenes.\n",
    "\n",
    "# The first thing we need to accomplish in this notebook is to find a way to, given\n",
    "# an arbitrary image, identify possible cards in it. Looks hard? It is hard to make\n",
    "# hard things easy, but give me a try :)\n",
    "\n",
    "\n",
    "# This method is a generator that will try to identify and isolate card-like rectangles\n",
    "# from a given image. All the returned rectangles are contained in the image, and they\n",
    "# are returned as binary images that have been straightened using an affinity matrix\n",
    "def get_potential_cards(image, dst_width=200, dst_height=283, card_min_image_area_percent=0.03, roi_min_image_area_percent=0.001):\n",
    "    # First, let's binarize the image using Otsu's thresholding.\n",
    "    gray_image = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\n",
    "    _, thresh_image = cv.threshold(gray_image, 0, 255, cv.THRESH_BINARY + cv.THRESH_OTSU)\n",
    "    # Now, we get all possible contours in the binarized image.\n",
    "    contours, _ = cv.findContours(\n",
    "        thresh_image, cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Not all contours are potential cards: we want them to meet\n",
    "    # some conditions to avoid unnecesary computations.\n",
    "    best_card_contours = get_best_card_contours(\n",
    "        image, contours, card_min_image_area_percent)\n",
    "    while True:\n",
    "        best_card_contour = next(best_card_contours)\n",
    "        if best_card_contour is None:\n",
    "            break\n",
    "\n",
    "        # It seems that we've got a potential card in this contour, so\n",
    "        # let's assume it is so. The card could be rotated, skewed...\n",
    "        # We'll need to straighten the card so it's top-left corner matches\n",
    "        # the top-left corner of the result image, as training cards do.\n",
    "\n",
    "        # We start by isolating the ROI to a new image and getting some\n",
    "        # information like vertices, bounding rect area, etc.\n",
    "        roi_x, roi_y, roi_w, roi_h = cv.boundingRect(best_card_contour)\n",
    "        roi_thresh_image = thresh_image[roi_y:roi_y +\n",
    "                                        roi_h, roi_x:roi_x + roi_w]\n",
    "        roi_origin = [roi_x, roi_y]\n",
    "        roi_vertices = translate_vertices(get_vertices_from_poly(\n",
    "            get_poly_from_contour(best_card_contour)), roi_origin)\n",
    "\n",
    "        roi_area = roi_w * roi_h\n",
    "        roi_min_image_area = roi_area * roi_min_image_area_percent\n",
    "\n",
    "        # To figure out how to orient the card, we need to find some\n",
    "        # different corner. We'll get all contours inside the card,\n",
    "        # with the hope that some contour will be very very close to\n",
    "        # a corner. The corner with the closest contour to it will be\n",
    "        # the one that we'll use as the top-left corner, as stated in the former rules.\n",
    "\n",
    "        # Possible optimization: look only for contours that are in\n",
    "        # the immediate level to the outermost contour, so we reduce\n",
    "        # the amount of contours that we iterate over to get the different corner.\n",
    "        #\n",
    "        # roi_contours, roi_hierarchies = cv.findContours(\n",
    "        #   roi_thresh_image, cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE)\n",
    "        # roi_external_contour_idx, _ = max(enumerate(roi_contours), key=lambda contour: cv.contourArea(contour[1]))\n",
    "        # roi_contours = [contour for i, contour in enumerate(roi_contours) if roi_hierarchies[0][i][3] == roi_external_contour_idx]\n",
    "        #\n",
    "        # Potential problems:\n",
    "        # - Might reduce the chance to find the right corner, as we skip some contours\n",
    "        #   and the one that makes the corner different might be included :)\n",
    "        # - Might not be computationally worth\n",
    "\n",
    "        roi_contours, _ = cv.findContours(\n",
    "            roi_thresh_image, cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "        # Let's get the corner that has the closest contour to it by taking the distance\n",
    "        # of the biggest contours to all corners.\n",
    "        closest_roi_contour_to_vertex = [None for _ in roi_vertices]\n",
    "        min_distance_roi_contour_to_vertex = [None for _ in roi_vertices]\n",
    "        for roi_contour in roi_contours:\n",
    "            if cv.contourArea(roi_contour) < roi_min_image_area:\n",
    "                continue\n",
    "            roi_contour_center = get_contour_center(roi_contour)\n",
    "            for i, roi_vertex in enumerate(roi_vertices):\n",
    "                roi_contour_distance_to_vertex = fast_euclidean_distance(\n",
    "                    roi_contour_center, roi_vertex)\n",
    "                if closest_roi_contour_to_vertex[i] is None or min_distance_roi_contour_to_vertex[i] > roi_contour_distance_to_vertex:\n",
    "                    closest_roi_contour_to_vertex[i] = roi_contour\n",
    "                    min_distance_roi_contour_to_vertex[i] = roi_contour_distance_to_vertex\n",
    "\n",
    "        # The \"most useful\" vertices are the ones that have closest contours\n",
    "        # to them. In a french 52-card deck, there are two: the corners with\n",
    "        # the suit and rank. But we only need one of them, so let's get the one\n",
    "        # with the closest contour to it.\n",
    "        roi_useful_vertices_idx = [idx for idx, _ in sorted(\n",
    "            enumerate(min_distance_roi_contour_to_vertex), key=lambda x: x[1])]\n",
    "        roi_most_useful_vertex_idx = roi_useful_vertices_idx[0]\n",
    "        roi_most_useful_vertex = roi_vertices[roi_most_useful_vertex_idx]\n",
    "        roi_ordered_vertices = [roi_most_useful_vertex]\n",
    "\n",
    "        # Ok, so we already know which one will be the top-left corner. Now, we\n",
    "        # need to sort the rest.\n",
    "        roi_unordered_vertices = [\n",
    "            roi_vertices[(roi_most_useful_vertex_idx + i) % 4] for i in range(1, 4)]\n",
    "        # Vertices are contiguous in the array. This means that either the next or the\n",
    "        # previous vertex must be closer to the top-left vertex, as they form the upper\n",
    "        # edge of the card. Doing so, we've just figured out if vertices are sorted clockwise\n",
    "        # or counterclockwise.\n",
    "        if fast_euclidean_distance(roi_most_useful_vertex, roi_unordered_vertices[2]) < fast_euclidean_distance(roi_most_useful_vertex, roi_unordered_vertices[0]):\n",
    "            roi_unordered_vertices.reverse()\n",
    "        roi_ordered_vertices += roi_unordered_vertices\n",
    "\n",
    "        # We know have all our source vertices labeled. Let's transform the card to\n",
    "        # de-rotate, de-skew and de-everything it!\n",
    "        dst_vertices = [[0, 0], [dst_width, 0], [\n",
    "            dst_width, dst_height], [0, dst_height]]\n",
    "        transform_matrix = cv.getPerspectiveTransform(\n",
    "            np.float32(roi_ordered_vertices), np.float32(dst_vertices))\n",
    "        dst_image = cv.warpPerspective(\n",
    "            roi_thresh_image, transform_matrix, (dst_width, dst_height))\n",
    "        _, dst_thresh_image = cv.threshold(dst_image, 0, 255, cv.THRESH_BINARY + cv.THRESH_OTSU)\n",
    "        yield dst_thresh_image\n",
    "    yield None\n",
    "\n",
    "\n",
    "# This is an auxiliar method to filter out some contours that doesn't\n",
    "# look like cards at all.\n",
    "def get_best_card_contours(image, contours, card_min_image_area_percent):\n",
    "    image_height, image_width, _ = image.shape\n",
    "    image_area = image_width * image_height\n",
    "\n",
    "    valid_detailed_contours = []\n",
    "    for contour in contours:\n",
    "        # We want cards to have, at least, a moreless acceptable area\n",
    "        area = cv.contourArea(contour)\n",
    "        if area < card_min_image_area_percent * image_area:\n",
    "            continue\n",
    "        poly = get_poly_from_contour(contour)\n",
    "        # And we want them to have four vertices... seriously, is that too much to ask?!\n",
    "        if len(poly) != 4:\n",
    "            continue\n",
    "        valid_detailed_contours.append((contour, area))\n",
    "\n",
    "    # Let's assume the bigger an area is, the better. Of course it could be not,\n",
    "    # but we rely on cards being a considerable part of the image.\n",
    "    largest_detailed_contours = sorted(\n",
    "        valid_detailed_contours, key=lambda detailed_contour: detailed_contour[1], reverse=True)\n",
    "    for detailed_contour in largest_detailed_contours:\n",
    "        yield detailed_contour[0]\n",
    "    yield None\n",
    "\n",
    "\n",
    "# As soon as we move into a ROI's boundary rect, we need to\n",
    "# translate its vertices to match the new image's coordinate space\n",
    "def translate_vertices(vertices, origin):\n",
    "    return [vertex - origin for vertex in vertices]\n",
    "\n",
    "\n",
    "# Get a contour's center\n",
    "def get_contour_center(contour):\n",
    "    moment = cv.moments(contour)\n",
    "    return [int(moment[\"m10\"] / moment[\"m00\"]), int(moment[\"m01\"] / moment[\"m00\"])]\n",
    "\n",
    "\n",
    "# Calculates something similar to the Euclidean distance, but\n",
    "# without taking the root. This is useful if you don't need the\n",
    "# real Euclidean distance, but to compare distances that are also\n",
    "# computed with this method.\n",
    "def fast_euclidean_distance(vertex1, vertex2):\n",
    "    return (vertex1[0] - vertex2[0]) ** 2 + (vertex1[1] - vertex2[1]) ** 2\n",
    "\n",
    "\n",
    "# Approximates a polygon from a given contour\n",
    "def get_poly_from_contour(contour):\n",
    "    perimeter = cv.arcLength(contour, True)\n",
    "    return cv.approxPolyDP(contour, 0.05 * perimeter, True)\n",
    "\n",
    "\n",
    "# Get the vertices of a polygon. Just a convenience method, nothing else.\n",
    "def get_vertices_from_poly(poly):\n",
    "    return [raw_vertex[0] for raw_vertex in poly]\n",
    "\n",
    "\n",
    "# This method will get the list of cards and will perform the training on them.\n",
    "# The training consists of:\n",
    "#  - Getting a potential card of each of the template images\n",
    "#  - Compute SIFT and ORB features from it and store them\n",
    "# If any of these steps is already done, then we skip it to save time.\n",
    "def perform_training():\n",
    "    changed_cards = set()\n",
    "\n",
    "    # Those two are feature extractors that we'll use\n",
    "    sift = cv.SIFT_create()\n",
    "    orb = cv.ORB_create()\n",
    "\n",
    "    if not os.path.exists(training_path):\n",
    "        os.mkdir(training_path)\n",
    "\n",
    "    for card_name in cards:\n",
    "        print(f\"{card_name}: \", end=\"\")\n",
    "        template_card_path = get_template_card_path(card_name)\n",
    "        trained_card_path = get_trained_card_path(card_name)\n",
    "        if not os.path.exists(trained_card_path):\n",
    "            card_image = cv.imread(template_card_path)\n",
    "            trained_card_image = next(get_potential_cards(card_image))\n",
    "            if trained_card_image is None:\n",
    "                print(f\"No card found, aborting.\")\n",
    "                continue\n",
    "            cv.imwrite(trained_card_path, trained_card_image)\n",
    "            # Adding a card to this set means that, even if we've got previous\n",
    "            # features saved, we need to recompute them and overwrite the current\n",
    "            # file.\n",
    "            changed_cards.add(card_name)\n",
    "            print(\"Got a new card, \", end=\"\")\n",
    "        else:\n",
    "            print(\"Already got a card, \", end=\"\")\n",
    "            trained_card_image = cv.imread(trained_card_path)\n",
    "\n",
    "        features_file_path = get_features_file_path(card_name)\n",
    "        if not os.path.exists(features_file_path) or card_name in changed_cards:\n",
    "            print(\"computed new features.\")\n",
    "            _, sift_features = sift.detectAndCompute(\n",
    "                trained_card_image, None\n",
    "            )\n",
    "            _, orb_features = orb.detectAndCompute(\n",
    "                trained_card_image, None\n",
    "            )\n",
    "            np.savez_compressed(features_file_path, sift_features=sift_features,\n",
    "                                orb_features=orb_features)\n",
    "        else:\n",
    "            print(f\"computing features skipped.\")\n",
    "    print(\"Training complete.\")\n",
    "\n",
    "\n",
    "# Actually performs the training\n",
    "perform_training()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matching cards\n",
    "\n",
    "At this point, we have all we need to run playing card matching on arbitrary images. But first, we need to load all the stuff we have computed previously.\n",
    "First, let's load the features we have computed (for feature matching) as well as the trained card images (for template matching)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Load features & training cards\n",
    "###############################################################################\n",
    "\n",
    "\n",
    "training_features = {\n",
    "    \"sift_features\": dict(),\n",
    "    \"orb_features\": dict()\n",
    "}\n",
    "training_cards = {\n",
    "    \"regular\": dict(),\n",
    "    \"upsidedown\": dict()\n",
    "}\n",
    "\n",
    "for card_name in cards:\n",
    "    with np.load(get_features_file_path(card_name), allow_pickle=True) as raw_features:\n",
    "        training_features[\"sift_features\"][card_name] = np.copy(\n",
    "            raw_features[\"sift_features\"])\n",
    "        training_features[\"orb_features\"][card_name] = np.copy(\n",
    "            raw_features[\"orb_features\"])\n",
    "    training_cards[\"regular\"][card_name] = cv.cvtColor(cv.imread(get_trained_card_path(card_name)), cv.COLOR_BGR2GRAY)\n",
    "    training_cards[\"upsidedown\"][card_name] = cv.rotate(training_cards[\"regular\"][card_name], cv.ROTATE_180)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's define the utilities and abstract algorithms that we'll use to apply matching. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Abstract matching algorithms & utilities\n",
    "###############################################################################\n",
    "\n",
    "\n",
    "# SIFT and ORB feature extractors\n",
    "sift = cv.SIFT_create()\n",
    "orb = cv.ORB_create()\n",
    "\n",
    "\n",
    "# Flann feature matcher for SIFT\n",
    "# (see https://docs.opencv.org/4.x/dc/dc3/tutorial_py_matcher.html)\n",
    "FLANN_INDEX_KDTREE = 1\n",
    "flann_for_sift_index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)\n",
    "flann_for_sift = cv.FlannBasedMatcher(flann_for_sift_index_params, dict())\n",
    "\n",
    "\n",
    "# Flann feature matcher for ORB\n",
    "FLANN_INDEX_LSH = 6\n",
    "flann_for_orb_index_params = dict(algorithm=FLANN_INDEX_LSH,\n",
    "                                  table_number=6,\n",
    "                                  key_size=12,\n",
    "                                  multi_probe_level=1)\n",
    "flann_for_orb = cv.FlannBasedMatcher(flann_for_orb_index_params, dict())\n",
    "\n",
    "\n",
    "# Brute-force feature matcher for SIFT\n",
    "bf_for_sift = cv.BFMatcher()\n",
    "\n",
    "\n",
    "# Brute-force feature matcher for ORB\n",
    "bf_for_orb = cv.BFMatcher(cv.NORM_HAMMING, crossCheck=True)\n",
    "\n",
    "\n",
    "# For matchers that use K-nn, this filter allows to\n",
    "# choose the best matches from a given list. Each match\n",
    "# is given as the closest and second closest distance between\n",
    "# features (lower is better). A large difference between the first\n",
    "# and the second distance implies an unambiguous match.\n",
    "def pass_good_matches(matches, threshold=0.7):\n",
    "    good_matches = []\n",
    "    for n, m in matches:\n",
    "        if n.distance < m.distance * threshold:\n",
    "            good_matches.append([n, m])\n",
    "    return good_matches\n",
    "\n",
    "\n",
    "# Identity function\n",
    "def pass_all_matches(matches):\n",
    "    return matches\n",
    "\n",
    "\n",
    "# This function will use OpenCV to perform template matching.\n",
    "# It also works for computing the similarity between two images\n",
    "# of the same size, as in this case.\n",
    "def opencv_template_matcher(image, template, mode = cv.TM_CCOEFF_NORMED):\n",
    "    matrix = cv.matchTemplate(image, template, mode)\n",
    "    min_val, max_val, _, _ = cv.minMaxLoc(matrix)\n",
    "    if mode in [cv.TM_SQDIFF, cv.TM_SQDIFF_NORMED]:\n",
    "        return min_val\n",
    "    else:\n",
    "        return max_val\n",
    "\n",
    "\n",
    "# Algorithm to apply descriptor matching. For each image, it will take\n",
    "# a potential card, extract its features, compute the score of the matching\n",
    "# between it and every training card, and return the best match.\n",
    "def match_cards_by_features(query_image, features, feature_extractor, matcher, match_filter=pass_all_matches, match_scorer=len):\n",
    "    potential_cards = get_potential_cards(query_image)\n",
    "    while True:\n",
    "        potential_card = next(potential_cards)\n",
    "        if potential_card is None:\n",
    "            break\n",
    "\n",
    "        _, query_features = feature_extractor(potential_card)\n",
    "        card_matches = {\n",
    "            card_name: match_filter(\n",
    "                matcher(\n",
    "                    query_features, features[card_name]\n",
    "                ),\n",
    "            ) for card_name in cards\n",
    "        }\n",
    "        scored_card_matches = {\n",
    "            card_name: match_scorer(matches) for card_name, matches in card_matches.items()\n",
    "        }\n",
    "        match_name = max(scored_card_matches,\n",
    "                         key=lambda key: scored_card_matches[key])\n",
    "        match_score = scored_card_matches[match_name]\n",
    "        yield (potential_card, match_name, match_score)\n",
    "    yield None\n",
    "\n",
    "\n",
    "# Algorithm to apply template matching. For each image, it will take\n",
    "# a potential card, compute the score of the comparison between it and\n",
    "# every training card, and return the best match.\n",
    "def match_cards_by_template(query_image, training_cards, matcher, get_best):\n",
    "    potential_cards = get_potential_cards(query_image)\n",
    "    while True:\n",
    "        potential_card = next(potential_cards)\n",
    "        if potential_card is None:\n",
    "            break\n",
    "\n",
    "        scored_card_comparisons = {\n",
    "            f\"{card_name}.{orientation}\": matcher(potential_card, training_cards[orientation][card_name])\n",
    "            for orientation in [\"regular\", \"upsidedown\"]\n",
    "            for card_name in cards\n",
    "        }\n",
    "        match_name = get_best(scored_card_comparisons)\n",
    "        generic_match_name = match_name.split(\".\")[0]\n",
    "        match_score = scored_card_comparisons[match_name]\n",
    "        yield (potential_card, generic_match_name, match_score)\n",
    "    yield None\n",
    "\n",
    "\n",
    "# Algorithm for getting valid matches. As our algorithm to detect potential cards\n",
    "# is a generator (i.e. it will return a card each time it is called, until it is exhausted),\n",
    "# we need some criteria to stop getting potential cards after we get 'n' bad results in a row.\n",
    "def get_valid_card_matches(card_matches, score_validator, max_failed_attempts=3):\n",
    "    failed_attempts = 0\n",
    "    valid_card_matches = []\n",
    "    while True:\n",
    "        card_match = next(card_matches)\n",
    "        if card_match is None:\n",
    "            break\n",
    "\n",
    "        _, _, match_score = card_match\n",
    "        if not score_validator(match_score):\n",
    "            failed_attempts += 1\n",
    "            if failed_attempts >= max_failed_attempts:\n",
    "                break\n",
    "        else:\n",
    "            valid_card_matches.append(card_match)\n",
    "    return valid_card_matches\n",
    "\n",
    "\n",
    "# This method will get a list of matched cards and will display it in a more convenient way\n",
    "def show_card_matches(query_image, training_cards, card_matches):\n",
    "    plt.title(\"Query image\")\n",
    "    plt.imshow(cv.cvtColor(query_image, cv.COLOR_BGR2RGB))\n",
    "    plt.show()\n",
    "\n",
    "    if len(card_matches) == 0:\n",
    "        print(\"No matches found!\")\n",
    "    else:\n",
    "        for i, (card_image, card_name, match_score) in enumerate(card_matches):\n",
    "            plt.subplot(121)\n",
    "            plt.title(f\"Detected card\")\n",
    "            plt.imshow(card_image, cmap=\"gray\")\n",
    "            plt.subplot(122)\n",
    "            plt.title(f\"Best match: {card_name} (score: {round(match_score, 2)})\")\n",
    "            plt.imshow(training_cards[card_name], cmap=\"gray\")\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playing card detection using feature matching\n",
    "\n",
    "This is the final step. Here, we load a query image and we try to match all all the potential card. In this case, we'll use feature matching. As you might have noticed before, all the code is pluggable as it is mostly written into functions that follow some kind of interfaces. This way, you can also write your own functions that will be plugged in the abstract algorithms and modify their behaviour.\n",
    "\n",
    "There are a lot of possible combinations!\n",
    "\n",
    "If you don't feel confident about this code, just change `query_image_name` and run the code :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this to choose the image file from the query folder that will be used\n",
    "query_image_name = \"q15.jpg\"\n",
    "\n",
    "# Advanced parameters\n",
    "# -------------------------------------------\n",
    "# Feature extractor (choose between \"sift\" or \"orb\") [NOTE: SIFT is the best]\n",
    "feature_extractor = \"sift\"\n",
    "# Feature matcher (choose between \"bf\" or \"flann\") [NOTE: FLANN is the best]\n",
    "feature_matcher = \"flann\"\n",
    "# Match filter (it will receive all matches, and you can filter them out) [NOTE: it will be ignored when using ORB]\n",
    "def match_filter(matches): return pass_good_matches(matches, threshold=0.70)\n",
    "# Match scorer (it will receive all filtered matches, and you can decide how good they are) [NOTE: the bigger the value, the better]\n",
    "def match_scorer(matches): return len(matches)\n",
    "# Global result filter (you will receive a score from match scorer, and you'll have to decide if\n",
    "#      it's good enough to be a card by returning true [it's a card] or false [it's not])\n",
    "def global_result_filter(score): return score > 20\n",
    "# How many failed attempts must happen to stop trying to match cards (by default, 3)\n",
    "max_failed_attempts = 3\n",
    "\n",
    "# Do not touch here!\n",
    "# Here, we are going to compute some values for the algorithm\n",
    "# depending on your decisions\n",
    "if feature_extractor == \"sift\":\n",
    "    selected_features = training_features[\"sift_features\"]\n",
    "\n",
    "    def selected_feature_extractor(\n",
    "        potential_card): return sift.detectAndCompute(potential_card, None)\n",
    "    if feature_matcher == \"flann\":\n",
    "        def selected_feature_matcher(\n",
    "            features1, features2): return flann_for_sift.knnMatch(features1, features2, k=2)\n",
    "    else:\n",
    "        def selected_feature_matcher(\n",
    "            features1, features2): return bf_for_sift.knnMatch(features1, features2, k=2)\n",
    "else:\n",
    "    selected_features = training_features[\"orb_features\"]\n",
    "\n",
    "    def selected_feature_extractor(\n",
    "        potential_card): return orb.detectAndCompute(potential_card, None)\n",
    "    if feature_matcher == \"flann\":\n",
    "        def selected_feature_matcher(\n",
    "            features1, features2): return flann_for_orb.match(features1, features2)\n",
    "    else:\n",
    "        def selected_feature_matcher(\n",
    "            features1, features2): return bf_for_orb.match(features1, features2)\n",
    "            \n",
    "    # We don't need to filter matches if using ORB\n",
    "    match_filter = pass_all_matches\n",
    "\n",
    "# This part of the code is the abstract algorithm\n",
    "query_image = cv.imread(get_query_image_path(query_image_name))\n",
    "card_matches = match_cards_by_features(\n",
    "    query_image,\n",
    "    selected_features,\n",
    "    selected_feature_extractor,\n",
    "    selected_feature_matcher,\n",
    "    match_filter,\n",
    "    match_scorer\n",
    ")\n",
    "valid_card_matches = get_valid_card_matches(\n",
    "    card_matches, global_result_filter, max_failed_attempts)\n",
    "show_card_matches(query_image, training_cards[\"regular\"], valid_card_matches)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playing card detection using template matching\n",
    "\n",
    "Same as above, feel free to change everything.\n",
    "\n",
    "If you don't feel confident about this code, just change `query_image_name` and run the code :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this to choose the image file from the query folder that will be used\n",
    "query_image_name = \"q15.jpg\"\n",
    "\n",
    "# Advanced parameters\n",
    "# -------------------------------------------\n",
    "# Template matcher that will receive two images and must give a score to how similar they are\n",
    "# You usually want to change only the third param of this, as there are three possible options:\n",
    "# cv.TM_CCOEFF_NORMED, cv.TM_CCORR_NORMED and cv.TM_SQDIFF_NORMED, which all return a float value between\n",
    "# 0 and 1. Note that for CCOEFF and CCORR, higher values are better, but SQDIFF behaves the other way around.\n",
    "def template_matcher(image1, image2): return opencv_template_matcher(\n",
    "    image1, image2, cv.TM_CCOEFF_NORMED)\n",
    "# Function to decide the best score among all. If using SQDIFF, please change this to min instead of max\n",
    "def best_score_getter(scores): return max(\n",
    "    scores, key=lambda card_name: scores[card_name])\n",
    "# Global result filter (you will receive a score from match scorer, and you'll have to decide if\n",
    "#      it's good enough to be a card by returning true [it's a card] or false [it's not])\n",
    "# If using SQDIFF, lower values are better, so you should flip the inequality sign :)\n",
    "def global_result_filter(score): return score > 0.3\n",
    "# How many failed attempts must happen to stop trying to match cards (by default, 3)\n",
    "max_failed_attempts = 3\n",
    "\n",
    "# This part of the code is the abstract algorithm\n",
    "query_image = cv.imread(get_query_image_path(query_image_name))\n",
    "card_matches = match_cards_by_template(\n",
    "    query_image,\n",
    "    training_cards,\n",
    "    template_matcher,\n",
    "    best_score_getter\n",
    ")\n",
    "valid_card_matches = get_valid_card_matches(\n",
    "    card_matches, global_result_filter, max_failed_attempts)\n",
    "show_card_matches(query_image, training_cards[\"regular\"], valid_card_matches)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
